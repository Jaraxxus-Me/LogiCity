# General simulation settings
simulation:
  map_yaml_file: "config/maps/square_5x5.yaml"       # OpenAI Gym environment name
  agent_yaml_file: "config/agents/easy_med/train.yaml" # Agents in the simulation
  ontology_yaml_file: "config/rules/ontology.yaml" # Ontology of the simulation
  rule_type: "Z3_RL"               # z3 rl will set the rl_agent with fixed number of other entities, return the groundings as obs, and return the rule reward
  rule_yaml_file: "config/rules/Nav/easy_med/rl.yaml"                    # Whether to render the environment
  rl: true
  debug: false
  use_multi: false
  agent_region: 100
  rl_agent: 
    max_priority: 6
    use_expert: false
    agent_name: "Car_1"               # ID of the RL agent, this is the agent id in the agent yaml file
    max_horizon: 200                          # Maximum steps in each episode 
    action_space: 4                       # Number of discrete actions, 4 means slow/normal/fast/stop
    action_mapping:                       # Mapping from policy action index to actual action space, see /agent folder
      0: [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
      1: [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0]
      2: [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0]
      3: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
    fov_entities:
      Entity: 5                        # Maximum number of entities in the FOV, include the RL agent
    action_cost:
      0: -2
      1: 0
      2: -2
      3: -5
    reset_dist:
      concepts: ['ambulance', 'bus', 'police', 'normal']
      prob: [0.25, 0.25, 0.25, 0.25]
    overtime_cost: -10

eval_checkpoint:
  eval_freq: 10000                        # This is the frequency of evaluation, will be multiplied by num_envs
  save_freq: 10000
  simulation_config:
    map_yaml_file: "config/maps/square_5x5.yaml"       # OpenAI Gym environment name
    agent_yaml_file: "config/agents/easy_med/val.yaml" # Agents in the simulation
    ontology_yaml_file: "config/rules/ontology.yaml" # Ontology of the simulation
    rule_type: "Z3_RL"               # z3 rl will set the rl_agent with fixed number of other entities, return the groundings as obs, and return the rule reward
    rule_yaml_file: "config/rules/Nav/easy_med/rl.yaml"                    # Whether to render the environment
    rl: true
    debug: false
    use_multi: false
    agent_region: 100
    rl_agent: 
      max_priority: 8
      use_expert: false
      agent_name: "Car_1"               # ID of the RL agent, this is the agent id in the agent yaml file
      max_horizon: 400                          # Maximum steps in each episode 
      action_space: 4                       # Number of discrete actions, 4 means slow/normal/fast/stop
      action_mapping:                       # Mapping from policy action index to actual action space, see /agent folder
        0: [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
        1: [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0]
        2: [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0]
        3: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
      fov_entities:
        Entity: 5                        # Maximum number of entities in the FOV, include the RL agent
      action_cost:
        0: -2
        1: 0
        2: -2
        3: -5
  save_path: './checkpoints/'
  name_prefix: 'easy_med_pponlm'
  episode_data: "dataset/easy_med/val_40_episodes.pkl"

# Stable Baselines specific settings
stable_baselines:
  algorithm: "NLMPPO"
  policy_network: "NLMPolicy"
  policy_kwargs:
    features_extractor_module: "logicity.rl_agent.policy.nlm_rl" # Module path
    features_extractor_class: "NLM"
    features_extractor_kwargs:
      tgt_arity: 1 # determines the feature axis
      nlm_args:
        input_dims: [0, 11, 6, 0] # the same as input_dims
        output_dims: 16 # the same as nlm_attributes
        logic_hidden_dim: []
        exclude_self: true
        depth: 4
        breadth: 3
        io_residual: false
        residual: false
        recursion: false
      num_ents: 5
      pred_grounding_index:
        IsPedestrian: [0, 5]
        IsCar: [5, 10]
        IsAmbulance: [10, 15]
        IsBus: [15, 20]
        IsPolice: [20, 25]
        IsTiro: [25, 30]
        IsReckless: [30, 35]
        IsOld: [35, 40]
        IsYoung: [40, 45]
        IsAtInter: [45, 50]
        IsInInter: [50, 55]
        IsClose: [55, 80]
        HigherPri': [80, 105]
        CollidingClose: [105, 130]
        LeftOf: [130, 155]
        RightOf: [155, 180]
        NextTo: [180, 205]
  hyperparameters:
    learning_rate: 0.00003          # Learning rate
    clip_range: 0.2              # Clip range for PPO
    vf_coef: 0.6                  # Value function coefficient
    batch_size: 256                # Batch size for training
    n_steps: 512                 # Number of steps to run for each environment per update
    n_epochs: 4                  # Number of epochs to run for each update
    ent_coef: 0.01                 # Entropy coefficient
    verbose: 1                    # Verbosity level
    tensorboard_log: "./tb_logs"  # Tensorboard log directory
  train: true                   # Whether to train the model
  checkpoint_path: ""           # Path to a checkpoint to restore the model from
  num_envs: 2                   # Number of environments to run in parallel
  total_timesteps: 1000000    # Total number of timesteps to run for all environments combined
