# General simulation settings
simulation:
  map_yaml_file: "config/maps/square_5x5.yaml"       # OpenAI Gym environment name
  agent_yaml_file: "config/agents/expert/val.yaml" # Agents in the simulation
  ontology_yaml_file: "config/rules/ontology_full.yaml" # Ontology of the simulation
  rule_type: "Z3_Expert"               # z3 rl will set the rl_agent with fixed number of other entities, return the groundings as obs, and return the rule reward
  rule_yaml_file: "config/rules/Nav/expert/expert.yaml"                    # Whether to render the environment
  rl: true
  debug: false
  use_multi: false
  agent_region: 120
  rl_agent: 
    max_priority: 9
    use_expert: true
    agent_name: "Car_1"               # ID of the RL agent, this is the agent id in the agent yaml file
    max_horizon: 10000                          # Maximum steps in each episode 
    action_space: 4                       # Number of discrete actions, 4 means slow/normal/fast/stop
    action_mapping:                       # Mapping from policy action index to actual action space, see /agent folder
      0: [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
      1: [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0]
      2: [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0]
      3: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
    fov_entities:
      Entity: 5                        # Maximum number of agents in the FOV, include the RL agent
    action_cost:
      0: -2
      1: -1
      2: -2
      3: -3

# Stable Baselines specific settings
stable_baselines:
  algorithm: "NLM"
  policy_network: "NLMPolicy"
  policy_kwargs:
    tgt_arity: 1
    nlm_args:
      input_dims: [0, 11, 6, 0] # the same as input_dims
      output_dims: 8 # the same as nlm_attributes
      logic_hidden_dim: []
      exclude_self: true
      depth: 4
      breadth: 3
      io_residual: false
      residual: false
      recursion: false
    target_dim: 4
  hyperparameters:
    index2action: {0: 'Slow', 1: 'Normal', 2: 'Fast', 3: 'Stop'}
    action2idx: {'Slow': 0, 'Normal': 1, 'Fast': 2, 'Stop': 3}
  train: false                   # Whether to train the model
  checkpoint_path: ""           # Path to a checkpoint to restore the model from
  num_envs: 1                   # Number of environments to run in parallel
  episode_data: "dataset/expert/val_40_episodes.pkl"
  eval_actions: {"Stop": 3, "Fast": 2, "Slow": 0}
