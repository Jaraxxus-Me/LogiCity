# General simulation settings
simulation:
  map_yaml_file: "config/maps/square_5x5.yaml"       # OpenAI Gym environment name
  agent_yaml_file: "config/agents/3p6c.yaml" # Agents in the simulation
  rule_type: "Z3_Expert"               # z3 rl will set the rl_agent with fixed number of other entities, return the groundings as obs, and return the rule reward
  rule_yaml_file: "config/rules/task/easy_expert.yaml"                    # Whether to render the environment
  rl: true
  debug: false
  use_multi: false
  agent_region: 100
  rl_agent: 
    use_expert: true
    agent_name: "Car_1"               # ID of the RL agent, this is the agent id in the agent yaml file
    max_horizon: 200                          # Maximum steps in each episode 
    action_space: 4                       # Number of discrete actions, 4 means slow/normal/fast/stop
    action_mapping:                       # Mapping from policy action index to actual action space, see /agent folder
      0: [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
      1: [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0]
      2: [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0]
      3: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
    fov_entities:
      Agent: 5                        # Maximum number of agents in the FOV, include the RL agent
      Intersection: 1                 # Maximum number of intersections in the FOV, these correspond to the definition of rules

# Stable Baselines specific settings
collecting_config:
  num_episodes: 10000    # Total number of episodes to collect data for.
  return_full_world: False